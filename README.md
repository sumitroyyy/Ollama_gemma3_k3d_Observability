# Ollama_gemma3_k3d_Observability
Run gemma3:2b locally with ollama and k3d and setup Observability for ollama containers 

This project demonstrates how to:

Deploy Ollama in a Kubernetes cluster

Expose metrics for Prometheus scraping

Visualize them in Grafana

Solve common real-world issues like exporter connectivity, scraping errors, and memory limits

Everything here is tested on k3d (k3s) but works on any Kubernetes cluster.

Features

Ollama model deployment in Kubernetes

Sidecar-based Prometheus Exporter for metrics

Annotation-based scraping for Prometheus

Grafana dashboards for real-time observability

Documented problems + solutions for production readiness

üìÇ Project Structure
ollama-monitoring/
‚îú‚îÄ‚îÄ ollama-deployment.yaml         # Ollama deployment with exporter sidecar
‚îú‚îÄ‚îÄ ollama1-pvc.yaml               # Persistent Volume Claim for model storage
‚îú‚îÄ‚îÄ prometheus-config-patch.yaml   # Patch for Prometheus scrape config

  Prerequisites

Kubernetes cluster (k3d, Minikube, or any K8s)

kubectl and helm CLI tools installed

4GB+ memory available (for the Gemma 2B model)

  Step 1: Install Prometheus & Grafana via Helm
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# Install Prometheus
helm install prometheus prometheus-community/prometheus -n monitoring --create-namespace

# Install Grafana
helm install grafana grafana/grafana -n monitoring --create-namespace


Check Pods:

kubectl get pods -n monitoring


Port-forward UIs:

kubectl port-forward svc/prometheus-server 9090:80 -n monitoring
kubectl port-forward svc/grafana 3000:80 -n monitoring


Grafana ‚Üí http://localhost:3000

Default user/pass: admin / admin

  Step 2: Deploy Ollama with PVC
2.1 Persistent Storage for Models
kubectl apply -f ollama1-pvc.yaml


This ensures models are stored persistently and don‚Äôt need to be re-downloaded on pod restarts.

2.2 Ollama Deployment with Exporter Sidecar
kubectl apply -f ollama-deployment.yaml


The main container: Runs Ollama server

Sidecar container: Runs Prometheus Exporter that converts Ollama JSON metrics ‚Üí Prometheus format

  Step 3: Patch Prometheus for Scraping

By default, Prometheus won‚Äôt scrape Ollama‚Äôs metrics.

kubectl patch configmap prometheus-server -n monitoring --patch-file prometheus-config-patch.yaml
kubectl rollout restart deployment prometheus-server -n monitoring


Check Targets ‚Üí Prometheus ‚Üí Status ‚Üí Targets ‚Üí should show Ollama as "UP".

  Problems Faced & Fixes

Here‚Äôs a complete list of real-world issues and how we fixed them:

1Ô∏è‚É£ Container Crash Loop: Invalid --host Flag

Problem:
Ollama crashed with:

Error: unknown flag: --host


Cause:
The --host flag was deprecated.

Fix:
Use environment variable instead:

env:
  - name: OLLAMA_HOST
    value: "0.0.0.0:11434"

2Ô∏è‚É£ Memory Limit Errors (OOMKilled, Exit Code 137)

Problem:
Model pod restarted with OOMKilled errors.

Cause:
Initial memory limits (1GB) too low for Gemma 2B model (1.7GB).

Fix:
Updated resource limits in ollama-deployment.yaml:

resources:
  requests:
    memory: "2Gi"
    cpu: "1000m"
  limits:
    memory: "4Gi"
    cpu: "4000m"

3Ô∏è‚É£ Prometheus Scrape Error: unsupported Content-Type "application/json"

Problem:
Prometheus expects text-based metrics, Ollama gives JSON.

Solution:

Added Ollama Exporter sidecar to convert JSON ‚Üí Prometheus format.

Used correct exporter image:

image: lucabecker42/ollama-exporter
args:
  - "--ollama.url=http://localhost:11434"
  - "--web.listen-address=:8000"


Scraped :8000/metrics instead of Ollama API.

4Ô∏è‚É£ Port Mismatch: Prometheus Target Down

Problem:
Exporter exposed port 8000, but Prometheus annotations scraped 9090.

Fix:
Match both:

annotations:
  prometheus.io/scrape: "true"
  prometheus.io/path: /metrics
  prometheus.io/port: "8000"

  Step 4: Grafana Dashboards

Add Prometheus as a data source ‚Üí http://prometheus-server.monitoring.svc.cluster.local

Import Kubernetes or Node Exporter Dashboards for instant metrics

Add custom panels for:

Memory vs Limits

CPU usage

Pod restarts

API latency

  Useful Commands

Check pod logs:

kubectl logs -n models -l app=ollama -f


Check events sorted by time:

kubectl get events -n models --sort-by='.lastTimestamp'


Debug inside container:

kubectl exec -it -n models <pod-name> -- /bin/sh

  Final Working Setup

Model: Gemma 2B

API: http://ollama-service.models:11434

Prometheus Scraping: Working via sidecar exporter

Grafana Dashboards: Real-time metrics

üóù Key Learnings

Exporter sidecar pattern is best for metrics format conversion

Memory planning is critical for AI models

Prometheus annotations must match the actual container ports

Environment variables > command-line flags for container config
